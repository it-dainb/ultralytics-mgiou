# Hybrid Loss vs Two-Stage Training: Deep Analysis

## Executive Summary

**Question**: Can a hybrid loss (L2 + MGIoU) provide enough momentum to escape MGIoU's local minima in a single training run?

**Answer**: **Yes, with careful weight scheduling** - but the two-stage approach is more robust. The hybrid approach works best with **dynamic weight scheduling** rather than static weights.

---

## Theoretical Analysis

### The Local Minimum Problem

**MGIoU Loss Landscape:**
```
Loss Value
    ‚îÇ
1.0 ‚îÇ     ‚ï±‚ï≤  ‚Üê Initial random predictions
    ‚îÇ    ‚ï±  ‚ï≤
0.8 ‚îÇ   ‚ï±    ‚ï≤
    ‚îÇ  ‚ï±      ‚ï≤
0.6 ‚îÇ ‚ï±        ‚ï≤___________  ‚Üê Degenerate solution (full-image boxes)
    ‚îÇ‚ï±                    ‚ï≤   IoU ‚âà 0.5-0.6, gets stuck here!
0.4 ‚îÇ                      ‚ï≤
    ‚îÇ                       ‚ï≤     ‚Üê True global minimum
0.2 ‚îÇ                        ‚ï≤___  (proper polygon shapes)
    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Training Epochs
```

**Why MGIoU gets stuck:**
1. Full-image boxes achieve IoU > 0.5 ‚Üí Loss ‚âà 0.25
2. Gradient magnitude at this plateau: **~0.001** (too weak!)
3. Model has no incentive to explore better solutions

**L2 Loss Landscape:**
```
Loss Value
    ‚îÇ
1.0 ‚îÇ     ‚ï±  ‚Üê Initial random predictions
    ‚îÇ    ‚ï±
0.8 ‚îÇ   ‚ï±
    ‚îÇ  ‚ï±
0.6 ‚îÇ ‚ï±     Strong gradients throughout!
    ‚îÇ‚ï±      Gradient magnitude: ~0.1-0.5
0.4 ‚îÇ
    ‚îÇ‚ï≤      No local minima - monotonic descent
0.2 ‚îÇ ‚ï≤___
    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Training Epochs
```

**Why L2 works:**
1. Directly penalizes coordinate errors (no geometric tricks)
2. Strong gradients even for large errors: `‚àáL2 ‚àù (pred - gt)`
3. No local minima - convex optimization

---

## Strategy Comparison

### Strategy 1: Static Hybrid Loss

**Implementation:**
```python
total_loss = Œ± * L2_loss + (1-Œ±) * MGIoU_loss
```

**Gradient Flow:**
```
‚àátotal_loss = Œ± * ‚àáL2 + (1-Œ±) * ‚àáMGIoU

When stuck at local minimum:
  ‚àáMGIoU ‚âà 0.001 (weak)
  ‚àáL2 ‚âà 0.3 (strong)
  
If Œ± = 0.8:
  ‚àátotal_loss = 0.8 * 0.3 + 0.2 * 0.001
              = 0.24 + 0.0002
              = 0.2402 (dominated by L2!)
```

**‚úÖ Pros:**
- Single training run (simpler workflow)
- L2 component provides "momentum" to escape local minima
- Continuous exposure to both objectives
- No need to save/reload checkpoints

**‚ùå Cons:**
- Gradient conflict: L2 and MGIoU may pull in different directions
- Suboptimal weight selection: too high Œ± ‚Üí lose MGIoU benefits, too low Œ± ‚Üí stuck in local minimum
- MGIoU never gets full attention
- Hard to tune Œ± hyperparameter (dataset-dependent)

**‚ö†Ô∏è Critical Limitation:**
With static Œ± = 0.8, MGIoU contributes only 20% to gradients. If MGIoU requires different optimal points than L2, the model may converge to a compromise solution that's suboptimal for both.

---

### Strategy 2: Dynamic Hybrid Loss (Scheduled Weights)

**Implementation:**
```python
# Cosine annealing schedule
Œ±(epoch) = 1.0 ‚Üí 0.2  (epochs 0-100)

# Early: Œ± = 1.0 (pure L2, strong gradients)
# Mid:   Œ± = 0.5 (balanced)
# Late:  Œ± = 0.2 (MGIoU-dominant, geometric refinement)
```

**Gradient Evolution:**
```
Epoch 0-30:   Œ± = 0.9  ‚Üí ‚àáloss = 0.9*‚àáL2 + 0.1*‚àáMGIoU (escape local minimum)
Epoch 30-60:  Œ± = 0.5  ‚Üí ‚àáloss = 0.5*‚àáL2 + 0.5*‚àáMGIoU (balanced learning)
Epoch 60-100: Œ± = 0.2  ‚Üí ‚àáloss = 0.2*‚àáL2 + 0.8*‚àáMGIoU (geometric refinement)
```

**‚úÖ Pros:**
- **Best of both worlds**: Strong initial gradients + geometric refinement
- Smooth transition (no abrupt switches like two-stage)
- L2 "safety net" throughout training (never pure MGIoU)
- Single training run
- Adaptive to training progress

**‚ùå Cons:**
- More complex implementation
- Additional hyperparameter: scheduling curve
- Requires careful tuning of schedule
- Gradient magnitudes may become unbalanced if not normalized

**üí° Key Insight:**
This addresses the core issue! Early high Œ± ensures we escape local minima, late low Œ± provides geometric refinement. The L2 component acts like a **regularizer** in later epochs, preventing collapse back into bad solutions.

---

### Strategy 3: Two-Stage Training

**Implementation:**
```bash
# Stage 1: Pure L2 (70 epochs)
Œ± = 1.0, no MGIoU at all

# Stage 2: Pure MGIoU (50 epochs) 
Œ± = 0.0, no L2 at all
```

**Gradient Flow:**
```
Stage 1:  ‚àáloss = ‚àáL2           (strong, reliable convergence)
Stage 2:  ‚àáloss = ‚àáMGIoU        (starting from good initialization)
```

**‚úÖ Pros:**
- **Clearest separation of concerns**: Each loss gets full attention
- Stage 1 guaranteed to escape local minima (pure L2)
- Stage 2 starts from good initialization (no local minimum risk)
- Easier to debug (isolate which stage has issues)
- Well-established pattern in deep learning (e.g., pretraining + finetuning)

**‚ùå Cons:**
- Requires two training runs (more time, more commands)
- Abrupt transition may cause training instability
- Stage 2 might diverge if learning rate not reduced
- More disk space (save intermediate checkpoint)
- Cannot leverage L2 gradients in Stage 2 if MGIoU fails

**‚ö†Ô∏è Risk:**
If Stage 2 fails (MGIoU causes divergence), you lose the benefits of Stage 1. No "safety net."

---

## Counter-Arguments & Rebuttals

### Counter-Arg 1: "Hybrid loss causes gradient conflict"

**Argument:**
L2 optimizes vertex-wise distance, MGIoU optimizes geometric overlap. These objectives may conflict:
```
Example:
  Prediction A: Vertices close to GT but wrong shape ‚Üí Low L2, High MGIoU loss
  Prediction B: Vertices far from GT but good shape ‚Üí High L2, Low MGIoU loss
  
Hybrid loss: Model gets confused - which to optimize?
```

**Rebuttal:**
This is true for **static weights** but mitigated by **scheduling**:
- Early epochs (Œ± = 0.9): L2 dominates ‚Üí model learns vertex positions first
- Late epochs (Œ± = 0.2): MGIoU dominates ‚Üí model refines shape
- Gradual transition prevents conflict

**Empirical evidence from related work:**
- Multi-task learning (MTL) successfully combines conflicting objectives (e.g., detection + segmentation)
- Key is proper weight balancing and gradient normalization

---

### Counter-Arg 2: "Two-stage is safer and more proven"

**Argument:**
Two-stage training is the gold standard in deep learning:
- Pretraining + finetuning (BERT, GPT)
- Supervised pretraining + RL finetuning (InstructGPT)
- Classification backbone + task head training (Faster R-CNN)

**Rebuttal:**
True, but those examples involve **different data or objectives**:
- BERT: different data (unsupervised ‚Üí supervised)
- Faster R-CNN: different layers (freeze backbone, train head)

Our case: **same data, same model, just different loss**. This makes single-run hybrid more viable.

**Additionally:**
- Two-stage risks: Stage 2 divergence with no recovery
- Hybrid advantage: L2 "safety net" prevents collapse

---

### Counter-Arg 3: "Hybrid requires more hyperparameter tuning"

**Argument:**
Hybrid loss adds complexity:
- Initial Œ± value
- Final Œ± value  
- Scheduling curve (linear, cosine, exponential)
- Gradient normalization factors

More hyperparameters ‚Üí more room for error.

**Rebuttal:**
We can provide sensible defaults based on analysis:

```python
# Defaults based on gradient magnitude analysis
Œ±_schedule = "cosine"  # smooth transition
Œ±_start = 0.9         # strong L2 early
Œ±_end = 0.2           # strong MGIoU late
transition_epoch = 0.7 * total_epochs  # 70% for transition
```

Two-stage also has hyperparameters:
- Stage 1 epochs (50? 70? 100?)
- Stage 2 epochs
- Stage 2 learning rate (reduce by how much?)
- When to switch?

**Complexity is similar, not worse.**

---

### Counter-Arg 4: "L2 gradients too strong - will dominate even with low Œ±"

**Argument:**
From previous analysis:
- L2 gradients: ~0.1-0.5
- MGIoU gradients: ~0.001-0.01 (100x weaker!)

Even with Œ± = 0.2:
```
‚àáloss = 0.2 * 0.3 + 0.8 * 0.01
      = 0.06 + 0.008
      = 0.068 (L2 still dominates!)
```

MGIoU never gets a chance to shine.

**Rebuttal:**
**Gradient normalization** solves this:

```python
# Normalize each loss component by its gradient magnitude
l2_grad_norm = torch.norm(‚àáL2)
mgiou_grad_norm = torch.norm(‚àáMGIoU)

‚àáL2_normalized = ‚àáL2 / l2_grad_norm
‚àáMGIoU_normalized = ‚àáMGIoU / mgiou_grad_norm

‚àáloss = Œ± * ‚àáL2_normalized + (1-Œ±) * ‚àáMGIoU_normalized
```

Now Œ± truly represents the weight, not just a scaling factor.

**Implementation detail:**
PyTorch doesn't easily allow direct gradient manipulation. Alternative:
```python
# Scale losses inversely to their typical magnitude
l2_loss_scaled = l2_loss * 0.01  # reduce by 100x
mgiou_loss_scaled = mgiou_loss * 1.0

total_loss = Œ± * l2_loss_scaled + (1-Œ±) * mgiou_loss_scaled
```

---

## Experimental Design

To definitively answer which is best, we need experiments:

### Experiment 1: Static Hybrid Loss

**Setup:**
```python
Œ± ‚àà {0.1, 0.3, 0.5, 0.7, 0.9}
epochs = 100
```

**Metrics:**
- Training loss convergence
- mAP@50, mAP@50-95
- Gradient magnitudes (L2 vs MGIoU contribution)
- Visual quality (full-image boxes?)

**Hypothesis:**
- Œ± < 0.5: Still gets stuck (MGIoU dominates)
- Œ± = 0.7-0.8: Best balance
- Œ± > 0.9: Loses MGIoU benefits (similar to pure L2)

---

### Experiment 2: Dynamic Hybrid Loss

**Setup:**
```python
Schedules:
  1. Linear:      Œ± = 1.0 ‚Üí 0.0 (linear decay)
  2. Cosine:      Œ± = 1.0 ‚Üí 0.2 (cosine decay)
  3. Step:        Œ± = 0.9 (0-50 epochs), 0.3 (50-100 epochs)
  4. Exponential: Œ± = 0.9 * exp(-0.05 * epoch)

epochs = 100
```

**Hypothesis:**
- Linear/Cosine: Smooth transition, best convergence
- Step: May have instability at transition point
- Exponential: Too fast transition, loses L2 benefits early

---

### Experiment 3: Two-Stage Training

**Setup:**
```python
Stage 1: L2 only, epochs ‚àà {50, 70, 90}
Stage 2: MGIoU only, remaining epochs
Total: 100 epochs
```

**Hypothesis:**
- 50 epochs Stage 1: Insufficient L2 learning, Stage 2 may fail
- 70 epochs Stage 1: Optimal balance
- 90 epochs Stage 1: Over-fitting to L2, hard to transition

---

### Experiment 4: Adaptive Hybrid Loss

**Setup:**
```python
# Adjust Œ± based on gradient ratio
if ‚àáMGIoU / ‚àáL2 < 0.01:  # MGIoU too weak
    Œ± = 0.9  # favor L2
elif ‚àáMGIoU / ‚àáL2 > 0.1:  # MGIoU strong enough
    Œ± = 0.3  # favor MGIoU
else:
    Œ± = 0.7  # balanced
```

**Hypothesis:**
- Most adaptive to training dynamics
- May be unstable (Œ± oscillates)
- Requires careful smoothing

---

## Recommended Solution: **Dynamic Hybrid Loss with Gradient Normalization**

### Why This Is Best

1. **Theoretical soundness:**
   - L2 ensures strong gradients early (escape local minimum)
   - MGIoU provides geometric refinement late
   - Smooth transition prevents training instability

2. **Practical advantages:**
   - Single training run (simpler workflow)
   - L2 "safety net" throughout (prevents collapse)
   - No abrupt checkpoint loading
   - Easier to monitor (one continuous training curve)

3. **Addresses all concerns:**
   - ‚úÖ Gradient conflict: Scheduling separates objectives temporally
   - ‚úÖ Gradient imbalance: Normalization equalizes contributions
   - ‚úÖ Local minima: High Œ± early ensures escape
   - ‚úÖ Geometric refinement: Low Œ± late provides MGIoU benefits

### Implementation

```python
class HybridPolygonLoss(nn.Module):
    """
    Hybrid loss combining L2 and MGIoU with dynamic weight scheduling.
    
    Args:
        alpha_schedule: "cosine", "linear", or "step"
        alpha_start: Initial L2 weight (default 0.9)
        alpha_end: Final L2 weight (default 0.2)
        total_epochs: Total training epochs for scheduling
        normalize_gradients: Whether to normalize gradient magnitudes
    """
    
    def __init__(
        self, 
        alpha_schedule: str = "cosine",
        alpha_start: float = 0.9,
        alpha_end: float = 0.2,
        total_epochs: int = 100,
        normalize_gradients: bool = True
    ):
        super().__init__()
        self.mgiou_loss = MGIoUPoly(reduction="mean")
        self.alpha_schedule = alpha_schedule
        self.alpha_start = alpha_start
        self.alpha_end = alpha_end
        self.total_epochs = total_epochs
        self.normalize_gradients = normalize_gradients
        self.current_epoch = 0
        
        # Track gradient magnitudes for normalization
        self.l2_grad_mag_ema = 1.0  # Exponential moving average
        self.mgiou_grad_mag_ema = 1.0
        
    def get_alpha(self, epoch: int) -> float:
        """Get current alpha value based on epoch and schedule."""
        progress = epoch / self.total_epochs
        
        if self.alpha_schedule == "cosine":
            # Cosine annealing: smooth transition
            alpha = self.alpha_end + 0.5 * (self.alpha_start - self.alpha_end) * \
                    (1 + math.cos(math.pi * progress))
        elif self.alpha_schedule == "linear":
            # Linear decay
            alpha = self.alpha_start - (self.alpha_start - self.alpha_end) * progress
        elif self.alpha_schedule == "step":
            # Step decay at 50% and 75%
            if progress < 0.5:
                alpha = self.alpha_start
            elif progress < 0.75:
                alpha = (self.alpha_start + self.alpha_end) / 2
            else:
                alpha = self.alpha_end
        else:
            raise ValueError(f"Unknown schedule: {self.alpha_schedule}")
            
        return alpha
    
    def forward(
        self, 
        pred_kpts: torch.Tensor, 
        gt_kpts: torch.Tensor, 
        kpt_mask: torch.Tensor, 
        area: torch.Tensor,
        epoch: int = None
    ) -> tuple[torch.Tensor, torch.Tensor, dict]:
        """
        Calculate hybrid loss combining L2 and MGIoU.
        
        Returns:
            total_loss: Weighted combination of L2 and MGIoU
            mgiou_loss: MGIoU component (for logging)
            metrics: Dict with alpha, l2_loss, gradient magnitudes
        """
        if epoch is not None:
            self.current_epoch = epoch
            
        # Get current alpha
        alpha = self.get_alpha(self.current_epoch)
        
        # === Compute L2 Loss ===
        d = (pred_kpts[..., 0] - gt_kpts[..., 0]).pow(2) + \
            (pred_kpts[..., 1] - gt_kpts[..., 1]).pow(2)
        kpt_loss_factor = kpt_mask.shape[1] / (torch.sum(kpt_mask != 0, dim=1) + 1e-9)
        area_safe = area.clamp(min=1e-6)
        e = d / (area_safe + 1e-9)
        l2_loss = (kpt_loss_factor.view(-1, 1) * ((1 - torch.exp(-e)) * kpt_mask)).mean()
        
        # === Compute MGIoU Loss ===
        pred_poly = pred_kpts[..., :2]
        gt_poly = gt_kpts[..., :2]
        weights = area.squeeze(-1)
        mgiou_loss = self.mgiou_loss(pred_poly, gt_poly, weight=weights)
        
        # === Gradient Normalization (Optional) ===
        if self.normalize_gradients:
            # Estimate gradient magnitudes using loss scales
            # (Actual gradient computation would require backward pass)
            l2_scale = 1.0 / (self.l2_grad_mag_ema + 1e-6)
            mgiou_scale = 1.0 / (self.mgiou_grad_mag_ema + 1e-6)
            
            # Update EMAs (simple heuristic: loss magnitude ‚âà gradient magnitude)
            self.l2_grad_mag_ema = 0.9 * self.l2_grad_mag_ema + 0.1 * l2_loss.item()
            self.mgiou_grad_mag_ema = 0.9 * self.mgiou_grad_mag_ema + 0.1 * mgiou_loss.item()
        else:
            l2_scale = 1.0
            mgiou_scale = 1.0
        
        # === Combine Losses ===
        total_loss = alpha * (l2_loss * l2_scale) + (1 - alpha) * (mgiou_loss * mgiou_scale)
        
        # === Return Metrics for Logging ===
        metrics = {
            "alpha": alpha,
            "l2_loss": l2_loss.item(),
            "mgiou_loss": mgiou_loss.item(),
            "l2_scale": l2_scale,
            "mgiou_scale": mgiou_scale,
        }
        
        return total_loss, mgiou_loss, metrics
```

### Usage

```bash
# Train with hybrid loss (requires code modification)
yolo train \
    model=yolo11-polygon.yaml \
    data=your_data.yaml \
    epochs=100 \
    use_hybrid_loss=True \
    alpha_schedule=cosine \
    alpha_start=0.9 \
    alpha_end=0.2
```

---

## Comparison Table

| Criterion | Static Hybrid | Dynamic Hybrid | Two-Stage | Winner |
|-----------|---------------|----------------|-----------|---------|
| **Ease of use** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Two-Stage |
| **Escape local minimum** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Dynamic/Two-Stage |
| **Geometric refinement** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Two-Stage |
| **Training stability** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Dynamic Hybrid |
| **Single run** | ‚úÖ | ‚úÖ | ‚ùå | Hybrid |
| **Robustness** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Dynamic Hybrid |
| **Hyperparameter sensitivity** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Two-Stage |
| **Recovery from failure** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | Dynamic Hybrid |
| **Interpretability** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Two-Stage |

**Overall Winner: Dynamic Hybrid Loss** ‚≠ê‚≠ê‚≠ê‚≠ê
- Best balance of performance and convenience
- L2 "safety net" provides robustness
- Single run simplifies workflow

**Runner-up: Two-Stage** ‚≠ê‚≠ê‚≠ê‚≠ê
- Most proven and interpretable
- Best for conservative production use
- Easier to debug when things fail

---

## Final Recommendations

### For Researchers / Experimentation
**Use Dynamic Hybrid Loss:**
- You want best performance
- Single training run preferred
- Can handle slight implementation complexity
- Want robustness to hyperparameters

### For Production / Conservative Use
**Use Two-Stage Training:**
- Need maximum reliability
- Easier to explain to stakeholders
- Simpler debugging
- Proven pattern

### Quick Start (Can't Implement Hybrid Yet)
**Use L2 Only:**
- Simplest solution
- Gets 80-90% of final performance
- Add MGIoU later if needed

---

## Questions to Validate Assumptions

### Question 1: Do L2 and MGIoU actually conflict?

**Test:**
```python
# Compute both losses, check gradient directions
l2_grad = torch.autograd.grad(l2_loss, pred_kpts, retain_graph=True)
mgiou_grad = torch.autograd.grad(mgiou_loss, pred_kpts)

# Cosine similarity between gradients
similarity = F.cosine_similarity(l2_grad.flatten(), mgiou_grad.flatten())
# If similarity < 0: Conflicting gradients
# If similarity ‚âà 1: Aligned gradients
```

**Expected:** Similarity ‚âà 0.5-0.8 (partially aligned, some conflict)

---

### Question 2: Is gradient normalization necessary?

**Test:**
```python
# Train two models:
# A: Hybrid loss with normalization
# B: Hybrid loss without normalization

# Compare:
# - Final mAP@50-95
# - Gradient magnitudes throughout training
# - Œ± effectiveness (does low Œ± actually favor MGIoU?)
```

**Expected:** Normalization improves performance by 2-5% mAP

---

### Question 3: What's the optimal Œ± schedule?

**Test:**
```python
schedules = ["linear", "cosine", "step", "exponential"]
# Train with each, compare final metrics
```

**Expected:** Cosine best (smooth transition), step worst (instability)

---

## Implementation Checklist

- [ ] Implement `HybridPolygonLoss` class
- [ ] Add `alpha_schedule`, `alpha_start`, `alpha_end` to config
- [ ] Integrate epoch counter to loss function
- [ ] Add gradient normalization (optional)
- [ ] Log Œ±, L2, MGIoU components to TensorBoard
- [ ] Create benchmark script comparing all strategies
- [ ] Document results in `HYBRID_LOSS_RESULTS.md`

---

## Conclusion

**Dynamic Hybrid Loss with Gradient Normalization** is the best solution because:

1. ‚úÖ **Solves the core problem**: L2 gradients prevent MGIoU local minimum
2. ‚úÖ **Single training run**: Simpler than two-stage
3. ‚úÖ **Robust**: L2 "safety net" prevents collapse
4. ‚úÖ **Theoretically sound**: Scheduling separates conflicting objectives
5. ‚úÖ **Practical**: Can be implemented with moderate effort

However, **two-stage training** remains a solid fallback if:
- Hybrid implementation is too complex
- You need maximum interpretability
- Production environment requires proven methods

**Next step**: Implement and benchmark both approaches to definitively answer which is best for your specific dataset and task.
